{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Force and) Honor Project\n",
    "\n",
    "## Short introduction\n",
    "\n",
    "Well, you know why we are here. In this notebook, I detail how I create a chatbot using first a multilayer bi-LSTM connected to a simple [Glove embeddings](https://nlp.stanford.edu/projects/glove/), and then maybe, if I have time, using a more complexe approach.\n",
    "\n",
    "First things first, let's start with the data.\n",
    "\n",
    "## Data Gathering and Preprocessing\n",
    "\n",
    "The first model will use Cornell dataset provided by the organiser. It's a small dataset so will give me probably a lesser quality bot, but it will be trained faster and will help for prototyping.\n",
    "Rather than making it download again on your machine, I will suppose it is on your machine under `./data/cornell`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:02<00:00, 30248.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "\n",
    "dataset_path = os.path.join(\"./data/cornell\")\n",
    "# Be mindful that I had to change the code in datasets.py for fast_preprocessing to \n",
    "# be actually taken into account\n",
    "data = datasets.readCornellData(dataset_path, max_len=20, fast_preprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24792\n",
      "[('there', 'where'), ('have fun tonight', 'tons'), ('what good stuff', 'the real you'), ('wow', 'lets go'), ('she okay', 'i hope so'), ('they do to', 'they do not'), ('who', 'joey'), ('its more', 'expensive'), ('hey sweet cheeks', 'hi joey'), ('whereve you been', 'nowhere hi daddy')]\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As written above, we are going to use Glove embeddings. We'll start by using the smallest version which is glove.6B.50d.txt, an 164M file of 400k vocabulary, 6B tokens projected into a 50 dimensions space (bigger versions go up to 840B tokens and 300 dimensions).\n",
    "\n",
    "Please note the treatment of special words: `<PAD>`, `<UNK>` and `<S>`. `<PAD>` is used as a padding word to put all the sentence at equal size (because tensor). `<UNK>` is a token replacing words which are not in the vocabulary. I deal here with out of vocabulary word by generating random vectors, which kind of treat them like noise. `<S>` is the starting token of the deoder. It doesn't need an embedding as the decoder output is a softmax on the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Constant values\n",
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\"\n",
    "START = \"<START>\"\n",
    "END = \"<EOS>\"\n",
    "\n",
    "class gloveEmbeddings:\n",
    "    def __init__(self):\n",
    "        self._embeddings = {}\n",
    "        self._id2word = []\n",
    "        self._word2id = {}\n",
    "        self._embeddings_dim = 0\n",
    "        \n",
    "    def load(self,filename,voc_size=0):\n",
    "        \"\"\"\n",
    "        Load the first voc_size words of a given glove size\n",
    "        if voc_size == 0, loads the whole file.\n",
    "        vocab is a set including the desired vocabulary. If not specified, we take everything\n",
    "        Returns:\n",
    "            embeddings: a dictionary word:embedding\n",
    "            word_order: a list containing the words in the same order as in the document\n",
    "            embeddings_dim: dimensionality of the embedding\n",
    "        \"\"\"\n",
    "        \n",
    "        # In case of multiple call\n",
    "        self._embeddings = {}\n",
    "        self._id2word = []\n",
    "        self._word2id = {}\n",
    "        self._embeddings_dim = 0\n",
    "        \n",
    "        self._id2word.append(PAD)\n",
    "        self._word2id[PAD] = 0\n",
    "        self._id2word.append(UNK)\n",
    "        self._word2id[UNK] = 1\n",
    "        self._id2word.append(START)\n",
    "        self._word2id[START] = 2\n",
    "        self._id2word.append(END)\n",
    "        self._word2id[END] = 3\n",
    "        count = 4\n",
    "        with open(filename,\"rt\") as f:\n",
    "            for line in f:\n",
    "                word,*proj = line.split()\n",
    "                self._embeddings[word] = np.array(proj,dtype=np.float32)\n",
    "                self._id2word.append(word)\n",
    "                self._word2id[word] = count\n",
    "                count += 1\n",
    "                if voc_size > 0 and count >= voc_size: break\n",
    "\n",
    "        self._embeddings_dim = len(next(iter(self._embeddings.values())))\n",
    "        self._embeddings[PAD] = np.zeros(self._embeddings_dim) # needed the dim\n",
    "        self._embeddings[START] = self._embeddings[PAD] + 1 # totally arbitrary\n",
    "        self._embeddings[END] = self._embeddings[PAD] - 1 # totally arbitrary\n",
    "        # note that UNK doesn't have an embedding, as it's a random vector generated at execution\n",
    "\n",
    "    def get(self,word):\n",
    "        \"\"\"\n",
    "        We'll deal with unknown words by returning a random vector\n",
    "        \"\"\"\n",
    "        if word not in self._embeddings or word == UNK:\n",
    "            return np.random.rand(self._embeddings_dim) * 2 - 1\n",
    "        return self._embeddings[word]\n",
    "    \n",
    "    def is_in(self,word):\n",
    "        return word in self._embeddings\n",
    "    \n",
    "    def word2id(self,word):\n",
    "        if word not in self._word2id:\n",
    "            return self._word2id[UNK]\n",
    "        return self._word2id[word]\n",
    "    \n",
    "    def id2word(self,index):\n",
    "        return self._id2word[index] # might trigger out of range\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._id2word)\n",
    "    \n",
    "    def get_dim(self):\n",
    "        return self._embeddings_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = gloveEmbeddings()\n",
    "glove.load(\"data/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove will be used in the input. As the output is only compared to the target sentences, we need to limit our output vocabulary to the set of words in the target vocabulary. We will also filter out words with less than a certain amount of occurences (most likely typos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "class outputVoc:\n",
    "    def __init__(self):\n",
    "        self._id2word = []\n",
    "        self._word2id = {}\n",
    "        self._word_freq = {}\n",
    "    \n",
    "    def learn_from_target(self,target_text,typo_limit=3):\n",
    "        self._word_freq = {}\n",
    "        for sen in target_text:\n",
    "            for word in sen.split():\n",
    "                if word not in self._word_freq:\n",
    "                    self._word_freq[word] = 1\n",
    "                else:\n",
    "                    self._word_freq[word] += 1\n",
    "        # Sanity check\n",
    "        max_word = \"\"\n",
    "        max_freq = 0\n",
    "        min_word = \"\"\n",
    "        min_freq = sys.maxsize\n",
    "        for word,freq in self._word_freq.items():\n",
    "            if freq > max_freq: \n",
    "                max_freq = freq\n",
    "                max_word = word\n",
    "            elif freq < min_freq:\n",
    "                min_freq = freq\n",
    "                min_word = word\n",
    "\n",
    "        print(\"Max freq word = \\\"{}\\\" : {}\".format(max_word,max_freq))\n",
    "        print(\"Min freq word = \\\"{}\\\" {}\".format(min_word,min_freq))\n",
    "        \n",
    "        self._id2word = []\n",
    "        self._word2id = {}\n",
    "        self._id2word.append(PAD)\n",
    "        self._word2id[PAD] = 0\n",
    "        self._id2word.append(UNK)\n",
    "        self._word2id[UNK] = 1\n",
    "        self._id2word.append(START)\n",
    "        self._word2id[START] = 2\n",
    "        self._id2word.append(END)\n",
    "        self._word2id[END] = 3\n",
    "        for word,freq in self._word_freq.items():\n",
    "            if freq >= typo_limit:\n",
    "                self._id2word.append(word)\n",
    "                self._word2id[word] = len(self._id2word) - 1\n",
    "        \n",
    "    def word2id(self,word):\n",
    "        if word not in self._word2id:\n",
    "            return self._word2id[UNK]\n",
    "        return self._word2id[word]\n",
    "    \n",
    "    def id2word(self,index):\n",
    "        return self._id2word[index] # can cause out of range exception\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._id2word)\n",
    "    \n",
    "    def words_to_one_hot(self,l_sen):\n",
    "        \"\"\"\n",
    "        I am keeping the tokenizer responsibility out, so only accept already tokenized sentences\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for word in l_sen:\n",
    "            one_hot = np.zeros(self.size())\n",
    "            indice = self.word2id(word)\n",
    "            one_hot[indice] = 1\n",
    "            out.append(one_hot)\n",
    "        return out\n",
    "    \n",
    "    def ids_to_one_hot(self,l_sen):\n",
    "        \"\"\"\n",
    "        I am keeping the tokenizer responsibility out, so only accept already tokenized sentences\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for indice in l_sen:\n",
    "            one_hot = np.zeros(self.size())\n",
    "            one_hot[indice] = 1\n",
    "            out.append(one_hot)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max freq word = \"you\" : 2850\n",
      "Min freq word = \"strapped\" 1\n",
      "Output vocabulary size 1726\n",
      "2 : <START>\n",
      "818 : you\n",
      "1254 : are\n",
      "441 : cute\n",
      "1 : <UNK>\n",
      "1 : <UNK>\n",
      "3 : <EOS>\n",
      "0 : <PAD>\n"
     ]
    }
   ],
   "source": [
    "out_voc = outputVoc()\n",
    "input_text,target_text = zip(*data)\n",
    "out_voc.learn_from_target(target_text)\n",
    "print(\"Output vocabulary size {}\".format(out_voc.size()))\n",
    "\n",
    "for x in out_voc.words_to_one_hot([\"<START>\",\"you\",\"are\",\"cute\",\",\",\"reviewer\",\"<EOS>\",\"<PAD>\"]):\n",
    "    argmax = np.argmax(x)\n",
    "    print(\"{} : {}\".format(argmax,out_voc.id2word(argmax)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that in tensor form and split training / testing (90 / 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM building\n",
    "\n",
    "Here come the big thing. Coding a bi-LSTM connected to the embeddings.\n",
    "First we define the tensorflow dataset as input with mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "#N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "lstm_dim = 128\n",
    "#units = 1024\n",
    "#vocab_inp_size = len(inp_lang.word2idx)\n",
    "#vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "embedding_matrix = []\n",
    "for i in range(glove.size()):\n",
    "    embedding_matrix.append(glove.get(glove.id2word(i)))\n",
    "embedding_matrix = np.matrix(embedding_matrix)\n",
    "\n",
    "#dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "#dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences        \n",
    "\n",
    "# replace words by their id in the tensor, higher in the network they will be replaced by their embeddings\n",
    "input_tensor,target_tensor = zip(*data)\n",
    "input_tensor = [[glove.word2id(word) for word in sentence.split()] for sentence in input_tensor]\n",
    "target_tensor = [[out_voc.word2id(word) for word in sentence.split()+[END]] for sentence in target_tensor]\n",
    "target_input_tensor = [[out_voc.word2id(START)] + line[:-1] for line in target_tensor]\n",
    "\n",
    "input_tensor = pad_sequences(input_tensor, maxlen=None,\n",
    "                             dtype='int32', padding='post', value=glove.word2id(PAD))\n",
    "target_tensor = pad_sequences(target_tensor, maxlen=None,\n",
    "                             dtype='int32', padding='post', value=out_voc.word2id(PAD))\n",
    "\n",
    "target_input_tensor = pad_sequences(target_input_tensor, maxlen=None,\n",
    "                             dtype='int32', padding='post', value=out_voc.word2id(PAD))\n",
    "\n",
    "# Remember that target_tensor must be a one-hot encoding\n",
    "\n",
    "\n",
    "# And now target tensor to one hot\n",
    "\n",
    "# train - eval split at 90-10\n",
    "#input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val, target_input_tensor_train, target_input_tensor_val = train_test_split(input_tensor, target_tensor, target_input_tensor, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  84    3    0    0    0    0    0    0]\n",
      " [ 848    3    0    0    0    0    0    0]\n",
      " [ 489 1052  818    3    0    0    0    0]]\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n",
      "where\n",
      "<EOS>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "====================\n",
      "tons\n",
      "<EOS>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "====================\n",
      "the\n",
      "real\n",
      "you\n",
      "<EOS>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "print(target_tensor[:3])\n",
    "target_tensor = np.array([out_voc.ids_to_one_hot(x) for x in target_tensor])\n",
    "print(target_tensor[:3])\n",
    "for i in range(3):\n",
    "    for w in target_tensor[i]:\n",
    "        print(out_voc.id2word(np.argmax(w)))\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, CuDNNLSTM, Embedding, Dropout, Dense\n",
    "\n",
    "class seq2Seq():\n",
    "    \n",
    "    def __init__(self,glove,out_voc):\n",
    "        self.glove = glove\n",
    "        self.out_voc = out_voc\n",
    "\n",
    "    def pretrained_embedding_layer(self):\n",
    "        \"\"\"\n",
    "        Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "\n",
    "        Arguments:\n",
    "        word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "        word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "        Returns:\n",
    "        embedding_layer -- pretrained layer Keras instance\n",
    "        \"\"\"\n",
    "\n",
    "        vocab_len = self.glove.size() + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "        emb_dim = self.glove.get_dim()      # define dimensionality of your GloVe word vectors (= 50)\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "        emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "\n",
    "        # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "        for word, index in self.glove._word2id.items():\n",
    "            emb_matrix[index, :] = self.glove.get(word)\n",
    "\n",
    "        # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "        self.embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "        self.embedding_layer.build((None,))\n",
    "\n",
    "        # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "        self.embedding_layer.set_weights([emb_matrix])\n",
    "        \n",
    "    def maybe_CuDNNLSTM(self,units,return_sequences=False,return_state=False):\n",
    "        if tf.test.is_gpu_available():\n",
    "            print(\"GPU acceleration available. LSTM cells will run faster\")\n",
    "            return CuDNNLSTM(units,return_sequences=return_sequences,return_state=return_state)\n",
    "        else:\n",
    "            print(\"GPU acceleration not available. LSTM cells will be slow\")\n",
    "            return LSTM(units,return_sequences=return_sequences,return_state=return_state)\n",
    "\n",
    "    def build_encoder(self,dropout=0.2):\n",
    "        self.encoder_inputs = Input(shape=(None,)) # it's an int\n",
    "\n",
    "        self.pretrained_embedding_layer()\n",
    "        #embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],input_length=7,trainable=False)\n",
    "        #embedding_layer.build((None,))\n",
    "        #embedding_layer.set_weights([embedding_matrix])\n",
    "        #embeddings = embedding_layer(sentence_indices)\n",
    "        self.input_embeddings = self.embedding_layer(self.encoder_inputs)\n",
    "\n",
    "        _,state_h,state_c = self.maybe_CuDNNLSTM(self.hidden_size,return_state=True)(self.input_embeddings)\n",
    "        state_h = Dropout(dropout)(state_h)\n",
    "        state_c = Dropout(dropout)(state_c)\n",
    "        self.encoder_states = [state_h,state_c]\n",
    "\n",
    "    \n",
    "    def build_training_model(self,hidden_size,dropout=0.2):\n",
    "        \n",
    "        units_out = self.out_voc.size()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.build_encoder(dropout=dropout)\n",
    "\n",
    "        self.decoder_inputs = Input(shape=(None,)) # also here it's an int\n",
    "        # IS IT NECESSARY?\n",
    "        #self.embedding_layer2 = self.pretrained_embedding_layer()\n",
    "        #embedding_layer2 = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],trainable=False)\n",
    "        #embedding_layer2.build((None,))\n",
    "        #embedding_layer2.set_weights([embedding_matrix])\n",
    "        self.decoder_input_embeddings = self.embedding_layer(self.decoder_inputs)\n",
    "        \n",
    "        self.decoder_lstm = self.maybe_CuDNNLSTM(hidden_size,return_sequences=True,return_state=True)\n",
    "        X,_,_ = self.decoder_lstm(self.decoder_input_embeddings,initial_state=self.encoder_states)\n",
    "        X = Dropout(dropout)(X)\n",
    "        self.decoder_dense = Dense(units_out, activation='softmax')\n",
    "        X = self.decoder_dense(X)\n",
    "\n",
    "        # Create Model instance which converts sentence_indices into X.\n",
    "        model = Model([self.encoder_inputs,self.decoder_inputs],X)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def build_decoder(self):\n",
    "        \n",
    "        state_input_h = Input(shape=(self.hidden_size,))\n",
    "        state_input_c = Input(shape=(self.hidden_size,))\n",
    "        self.decoder_states_inputs = [state_input_h, state_input_c]\n",
    "        \n",
    "        decoder_outputs, state_h, state_c = self.decoder_lstm(\n",
    "            self.decoder_input_embeddings, initial_state=self.decoder_states_inputs)\n",
    "        decoder_states = [state_h,state_c]\n",
    "        decoder_outputs = self.decoder_dense(decoder_outputs)\n",
    "        self.decoder_model = Model(\n",
    "            [self.decoder_inputs] + self.decoder_states_inputs,\n",
    "            [decoder_outputs] + decoder_states)\n",
    "        \n",
    "    def eval_seq(self,input_seq,max_length):\n",
    "        \n",
    "        encoder_model = Model(self.encoder_inputs, self.encoder_states)\n",
    "        seq = []\n",
    "        for w in input_seq.split():\n",
    "            seq.append(glove.word2id(w))\n",
    "        states_values = encoder_model.predict(seq)\n",
    "        target_seq = np.zeros((1, 1)) # we are using embedding indices as inputs\n",
    "        \n",
    "        target_seq[0, 0] = glove.word2id(START)\n",
    "        \n",
    "        decoded_sentence = \"\"\n",
    "        end_of_sentence = False\n",
    "        nb_words = 0\n",
    "        while nb_words < max_length:\n",
    "            output_tokens, h, c = self.decoder_model.predict(\n",
    "            [target_seq] + states_values)\n",
    "\n",
    "            # Sample a token\n",
    "            word = self.out_voc.id2word(np.argmax(output_tokens[0, -1, :]))\n",
    "            print(\"{} -> {} -> {}\".format(np.argmax(output_tokens[0, -1, :]),word,glove.word2id(word)))\n",
    "            nb_words += 1\n",
    "            decoded_sentence += \" \" + word\n",
    "            # Exit condition: either hit max length\n",
    "            # or find stop character.\n",
    "            if (word == END):\n",
    "                break\n",
    "\n",
    "            # Update the target sequence (of length 1).\n",
    "            target_seq[0, 0] = glove.word2id(word)\n",
    "\n",
    "            # Update states\n",
    "            states_value = [h, c]\n",
    "\n",
    "        return decoded_sentence\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU acceleration available. LSTM cells will run faster\n",
      "GPU acceleration available. LSTM cells will run faster\n"
     ]
    }
   ],
   "source": [
    "s2s = seq2Seq(glove,out_voc)\n",
    "model = s2s.build_training_model(lstm_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 50)     20000250    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)          [(None, 128), (None, 92160       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           cu_dnnlstm[0][1]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           cu_dnnlstm[0][2]                 \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, None, 128),  92160       embedding[1][0]                  \n",
      "                                                                 dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 128)    0           cu_dnnlstm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 1726)   222654      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 20,407,224\n",
      "Trainable params: 406,974\n",
      "Non-trainable params: 20,000,250\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24792, 8, 1726)\n",
      "(24792, 7)\n",
      "(24792, 8)\n"
     ]
    }
   ],
   "source": [
    "print(target_tensor.shape)\n",
    "print(input_tensor.shape)\n",
    "print(target_input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22312 samples, validate on 2480 samples\n",
      "Epoch 1/10\n",
      "22312/22312 [==============================] - 5s 242us/step - loss: 2.1070 - val_loss: 1.8620\n",
      "Epoch 2/10\n",
      "22312/22312 [==============================] - 3s 156us/step - loss: 1.8381 - val_loss: 1.8113\n",
      "Epoch 3/10\n",
      "22312/22312 [==============================] - 4s 169us/step - loss: 1.7905 - val_loss: 1.7714\n",
      "Epoch 4/10\n",
      "22312/22312 [==============================] - 4s 178us/step - loss: 1.7565 - val_loss: 1.7495\n",
      "Epoch 5/10\n",
      "22312/22312 [==============================] - 4s 168us/step - loss: 1.7336 - val_loss: 1.7313\n",
      "Epoch 6/10\n",
      "22312/22312 [==============================] - 4s 167us/step - loss: 1.7132 - val_loss: 1.7169\n",
      "Epoch 7/10\n",
      "22312/22312 [==============================] - 4s 159us/step - loss: 1.6981 - val_loss: 1.7073\n",
      "Epoch 8/10\n",
      "22312/22312 [==============================] - 4s 162us/step - loss: 1.6850 - val_loss: 1.7004\n",
      "Epoch 9/10\n",
      "22312/22312 [==============================] - 4s 160us/step - loss: 1.6728 - val_loss: 1.6884\n",
      "Epoch 10/10\n",
      "22312/22312 [==============================] - 3s 149us/step - loss: 1.6612 - val_loss: 1.6825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6f12b38cc0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[input_tensor,target_input_tensor], y=target_tensor,\n",
    "          batch_size=BATCH_SIZE,epochs=10,validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's try talking to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s.build_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1706 -> i -> 45\n",
      "3 -> <EOS> -> 3\n"
     ]
    }
   ],
   "source": [
    "sentence = s2s.eval_seq(\"what are you doing <EOS> <PAD> <PAD>\",7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXT show graph and see why loss is so low why results so shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
