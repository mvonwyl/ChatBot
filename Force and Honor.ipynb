{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Force and) Honor Project\n",
    "\n",
    "## Short introduction\n",
    "\n",
    "Well, you know why we are here. In this notebook, I detail how I create a chatbot using first a multilayer bi-LSTM connected to a simple [Glove embeddings](https://nlp.stanford.edu/projects/glove/), and then maybe, if I have time, using a more complexe approach.\n",
    "\n",
    "First things first, let's start with the data.\n",
    "\n",
    "## Data Gathering and Preprocessing\n",
    "\n",
    "The first model will use Cornell dataset provided by the organiser. It's a small dataset so will give me probably a lesser quality bot, but it will be trained faster and will help for prototyping.\n",
    "Rather than making it download again on your machine, I will suppose it is on your machine under `./data/cornell`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import chatterbot_interface\n",
    "from importlib import reload\n",
    "chatterbot_interface = reload(chatterbot_interface)\n",
    "from chatterbot_interface import load_chatterbot_data\n",
    "import os\n",
    "\n",
    "#dataset_path = \"./data/cornell\"\n",
    "# Be mindful that I had to change the code in datasets.py for fast_preprocessing to \n",
    "# be actually taken into account\n",
    "#data = datasets.readCornellData(dataset_path, max_len=20, fast_preprocessing=True)\n",
    "\n",
    "dataset_path = \"./data/bot-txt\"\n",
    "data = load_chatterbot_data(dataset_path,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745\n",
      "[('each year in pro baseball the', 'the gold glove'), ('if you are riding fakie inside', 'snowboarding'), ('what is basketball', 'a game with tall players'), ('what soccer', 'i was born without the sports gene'), ('what is baseball', 'a game played with a hard rawhide covered'), ('teams of nine or ten players each it', 'a diamond shaped circuit'), ('what is soccer', 'a game played with a round ball by'), ('a goal at either end the ball is', 'of the body except the hands and arms'), ('i love baseball', 'i am not into sports that much'), ('i play soccer', 'you have to run very fast to be')]\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As written above, we are going to use Glove embeddings. We'll start by using the smallest version which is glove.6B.50d.txt, an 164M file of 400k vocabulary, 6B tokens projected into a 50 dimensions space (bigger versions go up to 840B tokens and 300 dimensions).\n",
    "\n",
    "Please note the treatment of special words: `<PAD>`, `<UNK>` and `<S>`. `<PAD>` is used as a padding word to put all the sentence at equal size (because tensor). `<UNK>` is a token replacing words which are not in the vocabulary. I deal here with out of vocabulary word by generating random vectors, which kind of treat them like noise. `<S>` is the starting token of the deoder. It doesn't need an embedding as the decoder output is a softmax on the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Constant values\n",
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\"\n",
    "START = \"<START>\"\n",
    "END = \"<EOS>\"\n",
    "\n",
    "class gloveEmbeddings:\n",
    "    def __init__(self):\n",
    "        self._embeddings = {}\n",
    "        self._id2word = []\n",
    "        self._word2id = {}\n",
    "        self._embeddings_dim = 0\n",
    "        \n",
    "    def load(self,filename,voc_size=0):\n",
    "        \"\"\"\n",
    "        Load the first voc_size words of a given glove size\n",
    "        if voc_size == 0, loads the whole file.\n",
    "        vocab is a set including the desired vocabulary. If not specified, we take everything\n",
    "        Returns:\n",
    "            embeddings: a dictionary word:embedding\n",
    "            word_order: a list containing the words in the same order as in the document\n",
    "            embeddings_dim: dimensionality of the embedding\n",
    "        \"\"\"\n",
    "        \n",
    "        # In case of multiple call\n",
    "        self._embeddings = {}\n",
    "        self._id2word = []\n",
    "        self._word2id = {}\n",
    "        self._embeddings_dim = 0\n",
    "        \n",
    "        self._id2word.append(PAD)\n",
    "        self._word2id[PAD] = 0\n",
    "        self._id2word.append(UNK)\n",
    "        self._word2id[UNK] = 1\n",
    "        self._id2word.append(START)\n",
    "        self._word2id[START] = 2\n",
    "        self._id2word.append(END)\n",
    "        self._word2id[END] = 3\n",
    "        count = 4\n",
    "        with open(filename,\"rt\") as f:\n",
    "            for line in f:\n",
    "                word,*proj = line.split()\n",
    "                self._embeddings[word] = np.array(proj,dtype=np.float32)\n",
    "                self._id2word.append(word)\n",
    "                self._word2id[word] = count\n",
    "                count += 1\n",
    "                if voc_size > 0 and count >= voc_size: break\n",
    "\n",
    "        self._embeddings_dim = len(next(iter(self._embeddings.values())))\n",
    "        self._embeddings[PAD] = np.zeros(self._embeddings_dim) # needed the dim\n",
    "        self._embeddings[START] = self._embeddings[PAD] + 1 # totally arbitrary\n",
    "        self._embeddings[END] = self._embeddings[PAD] - 1 # totally arbitrary\n",
    "        # note that UNK doesn't have an embedding, as it's a random vector generated at execution\n",
    "\n",
    "    def get(self,word):\n",
    "        \"\"\"\n",
    "        We'll deal with unknown words by returning a random vector\n",
    "        \"\"\"\n",
    "        if word not in self._embeddings or word == UNK:\n",
    "            return np.random.rand(self._embeddings_dim) * 2 - 1\n",
    "        return self._embeddings[word]\n",
    "    \n",
    "    def is_in(self,word):\n",
    "        return word in self._embeddings\n",
    "    \n",
    "    def word2id(self,word):\n",
    "        if word not in self._word2id:\n",
    "            return self._word2id[UNK]\n",
    "        return self._word2id[word]\n",
    "    \n",
    "    def id2word(self,index):\n",
    "        return self._id2word[index] # might trigger out of range\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._id2word)\n",
    "    \n",
    "    def get_dim(self):\n",
    "        return self._embeddings_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = gloveEmbeddings()\n",
    "glove.load(\"data/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove will be used in the input. As the output is only compared to the target sentences, we need to limit our output vocabulary to the set of words in the target vocabulary. We will also filter out words with less than a certain amount of occurences (most likely typos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "class outputVoc:\n",
    "    def __init__(self):\n",
    "        self._id2word = []\n",
    "        self._word2id = {}\n",
    "        self._word_freq = {}\n",
    "    \n",
    "    def learn_from_target(self,target_text,typo_limit=3):\n",
    "        self._word_freq = {}\n",
    "        for sen in target_text:\n",
    "            for word in sen.split():\n",
    "                if word not in self._word_freq:\n",
    "                    self._word_freq[word] = 1\n",
    "                else:\n",
    "                    self._word_freq[word] += 1\n",
    "        # Sanity check\n",
    "        max_word = \"\"\n",
    "        max_freq = 0\n",
    "        min_word = \"\"\n",
    "        min_freq = sys.maxsize\n",
    "        for word,freq in self._word_freq.items():\n",
    "            if freq > max_freq: \n",
    "                max_freq = freq\n",
    "                max_word = word\n",
    "            elif freq < min_freq:\n",
    "                min_freq = freq\n",
    "                min_word = word\n",
    "\n",
    "        print(\"Max freq word = \\\"{}\\\" : {}\".format(max_word,max_freq))\n",
    "        print(\"Min freq word = \\\"{}\\\" {}\".format(min_word,min_freq))\n",
    "        \n",
    "        self._id2word = []\n",
    "        self._word2id = {}\n",
    "        self._id2word.append(PAD)\n",
    "        self._word2id[PAD] = 0\n",
    "        self._id2word.append(UNK)\n",
    "        self._word2id[UNK] = 1\n",
    "        self._id2word.append(START)\n",
    "        self._word2id[START] = 2\n",
    "        self._id2word.append(END)\n",
    "        self._word2id[END] = 3\n",
    "        for word,freq in self._word_freq.items():\n",
    "            if freq >= typo_limit:\n",
    "                self._id2word.append(word)\n",
    "                self._word2id[word] = len(self._id2word) - 1\n",
    "        \n",
    "    def word2id(self,word):\n",
    "        if word not in self._word2id:\n",
    "            return self._word2id[UNK]\n",
    "        return self._word2id[word]\n",
    "    \n",
    "    def id2word(self,index):\n",
    "        return self._id2word[index] # can cause out of range exception\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._id2word)\n",
    "    \n",
    "    def words_to_one_hot(self,l_sen):\n",
    "        \"\"\"\n",
    "        I am keeping the tokenizer responsibility out, so only accept already tokenized sentences\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for word in l_sen:\n",
    "            one_hot = np.zeros(self.size())\n",
    "            indice = self.word2id(word)\n",
    "            one_hot[indice] = 1\n",
    "            out.append(one_hot)\n",
    "        return out\n",
    "    \n",
    "    def ids_to_one_hot(self,l_sen):\n",
    "        \"\"\"\n",
    "        I am keeping the tokenizer responsibility out, so only accept already tokenized sentences\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for indice in l_sen:\n",
    "            one_hot = np.zeros(self.size())\n",
    "            one_hot[indice] = 1\n",
    "            out.append(one_hot)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max freq word = \"i\" : 262\n",
      "Min freq word = \"shame\" 1\n",
      "Output vocabulary size 247\n",
      "2 : <START>\n",
      "12 : you\n",
      "56 : are\n",
      "1 : <UNK>\n",
      "1 : <UNK>\n",
      "1 : <UNK>\n",
      "3 : <EOS>\n",
      "0 : <PAD>\n"
     ]
    }
   ],
   "source": [
    "out_voc = outputVoc()\n",
    "input_text,target_text = zip(*data)\n",
    "out_voc.learn_from_target(target_text)\n",
    "print(\"Output vocabulary size {}\".format(out_voc.size()))\n",
    "\n",
    "for x in out_voc.words_to_one_hot([\"<START>\",\"you\",\"are\",\"cute\",\",\",\"reviewer\",\"<EOS>\",\"<PAD>\"]):\n",
    "    argmax = np.argmax(x)\n",
    "    print(\"{} : {}\".format(argmax,out_voc.id2word(argmax)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that in tensor form and split training / testing (90 / 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM building\n",
    "\n",
    "Here come the big thing. Coding a bi-LSTM connected to the embeddings.\n",
    "First we define the tensorflow dataset as input with mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "embedding_matrix = []\n",
    "for i in range(glove.size()):\n",
    "    embedding_matrix.append(glove.get(glove.id2word(i)))\n",
    "embedding_matrix = np.matrix(embedding_matrix)\n",
    "\n",
    "#dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "#dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences        \n",
    "\n",
    "# replace words by their id in the tensor, higher in the network they will be replaced by their embeddings\n",
    "input_tensor,target_tensor = zip(*data)\n",
    "input_tensor = [[glove.word2id(word) for word in sentence.split()+[END]] for sentence in input_tensor]\n",
    "target_tensor = [[out_voc.word2id(word) for word in sentence.split()+[END]] for sentence in target_tensor]\n",
    "target_input_tensor = [[out_voc.word2id(START)] + line[:-1] for line in target_tensor]\n",
    "\n",
    "input_tensor = pad_sequences(input_tensor, maxlen=None,\n",
    "                             dtype='int32', padding='post', value=glove.word2id(PAD))\n",
    "target_tensor = pad_sequences(target_tensor, maxlen=None,\n",
    "                             dtype='int32', padding='post', value=out_voc.word2id(PAD))\n",
    "\n",
    "target_input_tensor = pad_sequences(target_input_tensor, maxlen=None,\n",
    "                             dtype='int32', padding='post', value=out_voc.word2id(PAD))\n",
    "\n",
    "# Remember that target_tensor must be a one-hot encoding\n",
    "\n",
    "\n",
    "# And now target tensor to one hot\n",
    "\n",
    "# train - eval split at 90-10\n",
    "#input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val, target_input_tensor_train, target_input_tensor_val = train_test_split(input_tensor, target_tensor, target_input_tensor, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     3, ...,    10,    66,   240],\n",
       "       [    0,     0,     3, ...,    36,    85,    87],\n",
       "       [    0,     0,     0, ...,  1792,    18,   106],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,     4,   840,    42],\n",
       "       [    0,     0,     0, ...,     4,   840,    42],\n",
       "       [    0,     0,     0, ..., 23560,   840,    42]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iinput_tensor = np.zeros(input_tensor.shape,dtype=int)\n",
    "for i in range(input_tensor.shape[0]):\n",
    "    for j in range(input_tensor.shape[1]):\n",
    "        iinput_tensor[i,j] = input_tensor[i,input_tensor.shape[1]-j-1]\n",
    "iinput_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[226   1   1   3   0   0   0   0   0]\n",
      " [  1   3   0   0   0   0   0   0   0]\n",
      " [139 160 240   1   1   3   0   0   0]]\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n",
      "the\n",
      "<UNK>\n",
      "<UNK>\n",
      "<EOS>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "====================\n",
      "<UNK>\n",
      "<EOS>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "====================\n",
      "a\n",
      "game\n",
      "with\n",
      "<UNK>\n",
      "<UNK>\n",
      "<EOS>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "print(target_tensor[:3])\n",
    "target_tensor = np.array([out_voc.ids_to_one_hot(x) for x in target_tensor])\n",
    "print(target_tensor[:3])\n",
    "for i in range(3):\n",
    "    for w in target_tensor[i]:\n",
    "        print(out_voc.id2word(np.argmax(w)))\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.layers import Input, LSTM, CuDNNLSTM, Embedding, Dropout, Dense, Masking\n",
    "from utils import BeamList\n",
    "import math\n",
    "\n",
    "class Seq2Seq():\n",
    "    \n",
    "    def __init__(self,glove,out_voc,depth=1):\n",
    "        self.glove = glove\n",
    "        self.out_voc = out_voc\n",
    "        self.depth=depth\n",
    "\n",
    "    def pretrained_embedding_layer(self):\n",
    "        \"\"\"\n",
    "        Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "\n",
    "        Arguments:\n",
    "        word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "        word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "        Returns:\n",
    "        embedding_layer -- pretrained layer Keras instance\n",
    "        \"\"\"\n",
    "\n",
    "        vocab_len = self.glove.size() + 1 # adding 1 to fit Keras embedding (requirement)\n",
    "        emb_dim = self.glove.get_dim()\n",
    "\n",
    "        emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "\n",
    "        for word, index in self.glove._word2id.items():\n",
    "            emb_matrix[index, :] = self.glove.get(word)\n",
    "\n",
    "        self.embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "        self.embedding_layer.build((None,))\n",
    "        self.embedding_layer.set_weights([emb_matrix])\n",
    "        \n",
    "    def maybe_CuDNNLSTM(self,units,return_sequences=False,return_state=False):\n",
    "        if tf.test.is_gpu_available():\n",
    "            print(\"GPU acceleration available. LSTM cells will run faster\")\n",
    "            return CuDNNLSTM(units,return_sequences=return_sequences,return_state=return_state)\n",
    "        else:\n",
    "            print(\"GPU acceleration not available. LSTM cells will be slow\")\n",
    "            return LSTM(units,return_sequences=return_sequences,return_state=return_state)\n",
    "\n",
    "    def build_encoder(self,sentence_length=8,dropout=0.2):\n",
    "        self.encoder_inputs = Input(shape=(None,)) # it's an int\n",
    "\n",
    "        self.pretrained_embedding_layer()\n",
    "        self.input_embeddings = self.embedding_layer(self.encoder_inputs)\n",
    "        \n",
    "        current_input = self.input_embeddings\n",
    "        self.encoder_states = []\n",
    "        for i in range(self.depth):\n",
    "            output,state_h,state_c = self.maybe_CuDNNLSTM(self.hidden_size,return_sequences=True,\n",
    "                                                          return_state=True)(current_input)\n",
    "            state_h = Dropout(dropout)(state_h)\n",
    "            state_c = Dropout(dropout)(state_c)\n",
    "            current_input = Dropout(dropout)(output)\n",
    "            self.encoder_states.append(state_h)\n",
    "            self.encoder_states.append(state_c)\n",
    "\n",
    "    \n",
    "    def build_training_model(self,hidden_size,dropout=0.2):\n",
    "        \n",
    "        units_out = self.out_voc.size()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.build_encoder(dropout=dropout)\n",
    "\n",
    "        self.decoder_inputs = Input(shape=(None,))\n",
    "        #self.decoder_masked_inputs = Masking(mask_value=self.glove.word2id(PAD), input_shape=(None))(self.decoder_inputs)\n",
    "        self.decoder_input_embeddings = self.embedding_layer(self.decoder_inputs)\n",
    "        \n",
    "        # Decoder at training time\n",
    "        \n",
    "        self.decoder_lstm = []\n",
    "        current_input = self.decoder_input_embeddings\n",
    "        X = None\n",
    "        for i in range(self.depth):\n",
    "            self.decoder_lstm.append(self.maybe_CuDNNLSTM(hidden_size,return_sequences=True,return_state=True))\n",
    "            X,_,_ = self.decoder_lstm[i](current_input,\n",
    "                                         initial_state=[self.encoder_states[2*i],self.encoder_states[2*i+1]])\n",
    "            X = Dropout(dropout)(X)\n",
    "            current_input = X\n",
    "        \n",
    "        self.decoder_dense = Dense(units_out, activation='softmax')\n",
    "        final_output = self.decoder_dense(current_input)\n",
    "\n",
    "        model = Model([self.encoder_inputs,self.decoder_inputs],final_output)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def build_decoder(self):\n",
    "        \"\"\"\n",
    "        Decoder at evaluation time\n",
    "        \"\"\"\n",
    "        \n",
    "        self.encoder_model = Model(self.encoder_inputs, self.encoder_states)\n",
    "        \n",
    "        # state inputs\n",
    "        decoder_states_inputs = []\n",
    "        for i in range(self.depth):\n",
    "            state_input_h = Input(shape=(self.hidden_size,))\n",
    "            state_input_c = Input(shape=(self.hidden_size,))\n",
    "            decoder_states_inputs.append(state_input_h)\n",
    "            decoder_states_inputs.append(state_input_c)\n",
    "        \n",
    "        # going through the layers\n",
    "        current_input = self.decoder_input_embeddings\n",
    "        decoder_states = []\n",
    "        for i in range(self.depth):\n",
    "            states_input = [decoder_states_inputs[2*i],decoder_states_inputs[2*i+1]]\n",
    "            decoder_outputs, state_h, state_c = self.decoder_lstm[i](\n",
    "                current_input, initial_state=states_input)\n",
    "            current_input = decoder_outputs\n",
    "            decoder_states.append(state_h)\n",
    "            decoder_states.append(state_c)\n",
    "        decoder_outputs = self.decoder_dense(decoder_outputs)\n",
    "            \n",
    "        self.decoder_model = Model(\n",
    "            [self.decoder_inputs] + decoder_states_inputs,\n",
    "            [decoder_outputs] + decoder_states)\n",
    "        \n",
    "    def eval_seq(self,input_seq,max_length,beam=10):\n",
    "        \n",
    "        seq = []\n",
    "        for w in input_seq.split():\n",
    "            seq.append(self.glove.word2id(w))\n",
    "        seq = [[seq]]\n",
    "        print(seq)\n",
    "        states_values = self.encoder_model.predict(seq)\n",
    "        print(len(states_values))\n",
    "        print(states_values[0].shape)\n",
    "        print(states_values[1].shape)\n",
    "        print(states_values[0][:,0])\n",
    "        print(states_values[1][:,0])\n",
    "        print(\"==============\")\n",
    "        target_seq = np.zeros((1, 1)) # we are using embedding indices as inputs\n",
    "        #states_values = sv\n",
    "        \n",
    "        # First run (with START token)\n",
    "        target_seq[0, 0] = self.glove.word2id(START)\n",
    "        output_tokens, *states_output = self.decoder_model.predict([target_seq] + states_values)\n",
    "        sorted_indices = np.argsort(output_tokens[0, 0, :])\n",
    "        beamer = BeamList(beam)\n",
    "        for b in range(beam): # beam update\n",
    "            indice = sorted_indices[-(b+1)]\n",
    "            new_id = self.glove.word2id(self.out_voc.id2word(indice))\n",
    "            beamer.insert((math.log(output_tokens[0,0,indice]),states_output,[new_id])) \n",
    "        nb_words = 1\n",
    "        \n",
    "        while nb_words < max_length:\n",
    "            # beam update\n",
    "            new_beamer = BeamList(beam)\n",
    "            for i in range(beam):\n",
    "                log_prob, states_values, sequence = beamer.get(i)\n",
    "                print(log_prob,end=\" \")\n",
    "                for s in sequence:\n",
    "                    print(self.glove.id2word(s), end=\" \")\n",
    "                print()\n",
    "                target_seq[0,0] = sequence[-1]\n",
    "                output_tokens, *states_output = self.decoder_model.predict(\n",
    "                [target_seq] + states_values)\n",
    "                # beam update\n",
    "                sorted_indices = np.argsort(output_tokens[0, 0, :])\n",
    "                for j in range(beam):\n",
    "                    indice = sorted_indices[-(j+1)]\n",
    "                    new_id = self.glove.word2id(self.out_voc.id2word(indice))\n",
    "                    new_beamer.insert((log_prob+math.log(output_tokens[0,0,indice]),states_output,sequence+[new_id]))\n",
    "            print(\"\\n===========\")\n",
    "            beamer = new_beamer\n",
    "            \n",
    "            nb_words += 1\n",
    "        \n",
    "        # decode best sequence\n",
    "        # @ADD stop computing sequence with EOS (dead seq with fixed prob at end)\n",
    "        # need to check if all sentences reached this point\n",
    "        \n",
    "        for i in range(beam):\n",
    "            decoded_sentence = \"\"\n",
    "            log_prob,_,sentence = beamer.get(i)\n",
    "            for s in sentence:\n",
    "                decoded_sentence += \" {}\".format(self.glove.id2word(s))\n",
    "            print(\"{} {}\".format(log_prob,decoded_sentence))\n",
    "\n",
    "        return \"\"\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745 9 247\n"
     ]
    }
   ],
   "source": [
    "samples,height,width = target_tensor.shape\n",
    "iinput_tmpsor = np.zeros((2000,height))\n",
    "target_tmpsor = np.zeros((2000,height,width))\n",
    "target_input_tmpsor = np.zeros((2000,height))\n",
    "for i in range(400):\n",
    "    for j in range(5):\n",
    "        iinput_tmpsor[i*5+j,:] = iinput_tensor[j,:]\n",
    "        target_tmpsor[i*5+j,:,:] = target_tensor[j,:,:]\n",
    "        target_input_tmpsor[i*5+j,:] = target_input_tensor[j,:]\n",
    "print(samples,height,width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU acceleration available. LSTM cells will run faster\n",
      "GPU acceleration available. LSTM cells will run faster\n",
      "GPU acceleration available. LSTM cells will run faster\n",
      "GPU acceleration available. LSTM cells will run faster\n",
      "GPU acceleration available. LSTM cells will run faster\n",
      "GPU acceleration available. LSTM cells will run faster\n",
      "Train on 670 samples, validate on 75 samples\n",
      "Epoch 1/50\n",
      "670/670 [==============================] - 7s 10ms/step - loss: 3.7020 - val_loss: 3.6588\n",
      "Epoch 2/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 3.3444 - val_loss: 3.5649\n",
      "Epoch 3/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 3.2612 - val_loss: 3.5847\n",
      "Epoch 4/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 3.2234 - val_loss: 3.6389\n",
      "Epoch 5/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 3.1845 - val_loss: 3.5474\n",
      "Epoch 6/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 3.1263 - val_loss: 3.4706\n",
      "Epoch 7/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 2.9397 - val_loss: 3.1290\n",
      "Epoch 8/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 2.6291 - val_loss: 3.1151\n",
      "Epoch 9/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 2.5842 - val_loss: 3.1907\n",
      "Epoch 10/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 2.5054 - val_loss: 3.1996\n",
      "Epoch 11/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 2.4787 - val_loss: 3.1721\n",
      "Epoch 12/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 2.4260 - val_loss: 3.1892\n",
      "Epoch 13/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 2.3764 - val_loss: 3.1994\n",
      "Epoch 14/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 2.3125 - val_loss: 3.2775\n",
      "Epoch 15/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 2.2624 - val_loss: 3.0624\n",
      "Epoch 16/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 2.1861 - val_loss: 2.9611\n",
      "Epoch 17/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 2.1353 - val_loss: 2.7648\n",
      "Epoch 18/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 2.0742 - val_loss: 2.8698\n",
      "Epoch 19/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 1.9887 - val_loss: 2.6478\n",
      "Epoch 20/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 1.9243 - val_loss: 2.5668\n",
      "Epoch 21/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 1.8399 - val_loss: 2.5570\n",
      "Epoch 22/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 1.7622 - val_loss: 2.4849\n",
      "Epoch 23/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 1.6680 - val_loss: 2.2967\n",
      "Epoch 24/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 1.5886 - val_loss: 2.2273\n",
      "Epoch 25/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 1.5056 - val_loss: 2.1399\n",
      "Epoch 26/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 1.3943 - val_loss: 2.0827\n",
      "Epoch 27/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 1.3219 - val_loss: 2.0582\n",
      "Epoch 28/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 1.2394 - val_loss: 1.9643\n",
      "Epoch 29/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 1.1624 - val_loss: 1.9370\n",
      "Epoch 30/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 1.0833 - val_loss: 1.8164\n",
      "Epoch 31/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 1.0215 - val_loss: 1.8473\n",
      "Epoch 32/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.9526 - val_loss: 1.7626\n",
      "Epoch 33/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.8782 - val_loss: 1.7038\n",
      "Epoch 34/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.8224 - val_loss: 1.7566\n",
      "Epoch 35/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.7749 - val_loss: 1.7354\n",
      "Epoch 36/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.7205 - val_loss: 1.7159\n",
      "Epoch 37/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.6966 - val_loss: 1.7286\n",
      "Epoch 38/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.6538 - val_loss: 1.6874\n",
      "Epoch 39/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.6218 - val_loss: 1.7245\n",
      "Epoch 40/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.5761 - val_loss: 1.6733\n",
      "Epoch 41/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.5517 - val_loss: 1.7490\n",
      "Epoch 42/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.5182 - val_loss: 1.6825\n",
      "Epoch 43/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.4845 - val_loss: 1.7375\n",
      "Epoch 44/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.4657 - val_loss: 1.7283\n",
      "Epoch 45/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.4502 - val_loss: 1.6680\n",
      "Epoch 46/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.4218 - val_loss: 1.7263\n",
      "Epoch 47/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.4041 - val_loss: 1.7283\n",
      "Epoch 48/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.3836 - val_loss: 1.7356\n",
      "Epoch 49/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.3622 - val_loss: 1.7585\n",
      "Epoch 50/50\n",
      "670/670 [==============================] - 5s 7ms/step - loss: 0.3461 - val_loss: 1.7732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f88f4686240>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to penalise <PAD>, <UNK> and <EOS> (for longer sentences and no UNK output)\n",
    "import keras\n",
    "from keras.utils import plot_model\n",
    "\n",
    "#BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 16\n",
    "#N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "lstm_dim = 512\n",
    "#units = 1024\n",
    "#vocab_inp_size = len(inp_lang.word2idx)\n",
    "#vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "\n",
    "s2s = Seq2Seq(glove,out_voc,depth=2)\n",
    "model = s2s.build_training_model(lstm_dim,dropout=0.3)\n",
    "plot_model(model, to_file='model.png')\n",
    "#model.summary()\n",
    "\n",
    "# class_weight doesn't work anyway\n",
    "class_weight = np.ones(out_voc.size())\n",
    "class_weight[out_voc.word2id(PAD)] = 0.001\n",
    "class_weight[out_voc.word2id(UNK)] = 0.001\n",
    "#class_weight[out_voc.word2id(END)] = 1\n",
    "\n",
    "callbacks = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0,write_graph=True, write_images=True)\n",
    "\n",
    "model.fit(x=[iinput_tensor,target_input_tensor], y=target_tensor,\n",
    "          batch_size=BATCH_SIZE,epochs=50,\n",
    "          validation_split=0.1,class_weight=class_weight,callbacks=[callbacks])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's try talking to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s.build_decoder()\n",
    "plot_model(s2s.encoder_model, to_file=\"encoder_model.png\")\n",
    "plot_model(s2s.decoder_model, to_file='eval_model.png')\n",
    "#s2s.decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 0, 0, 0, 3, 168476, 18, 106]]]\n",
      "6\n",
      "(1, 1024)\n",
      "(1, 1024)\n",
      "[0.04686449]\n",
      "[0.22465827]\n",
      "==============\n",
      "-1.120331726323114 a \n",
      "-1.2656580856106934 <UNK> \n",
      "-2.558579185618506 is \n",
      "-2.622851836881009 the \n",
      "-2.93099181457561 i \n",
      "-3.02710075883412 an \n",
      "-3.0611058466958934 he \n",
      "-4.228704329837161 not \n",
      "-4.229469685713195 thomas \n",
      "-4.451262926030143 hal \n",
      "-5.168568186037519 my \n",
      "-5.215365782084313 of \n",
      "-5.273697326699788 what \n",
      "-5.282078148975742 you \n",
      "-5.7858771845099515 do \n",
      "-6.077127437388312 it \n",
      "-6.396555309402902 to \n",
      "-6.444980062060247 no \n",
      "-6.611964126378443 by \n",
      "-6.687531420364789 in \n",
      "-6.88809764308106 history \n",
      "-7.001948332002698 how \n",
      "-7.030136181457714 have \n",
      "-7.0366925515107885 that \n",
      "-7.2414078516739115 or \n",
      "-7.3833727455827916 should \n",
      "-7.596174223623606 one \n",
      "-7.653330326958242 software \n",
      "-7.699461456345004 <EOS> \n",
      "-7.726842824456796 does \n",
      "\n",
      "===========\n",
      "-1.3664890806608472 <UNK> is \n",
      "-1.443146314759205 a <UNK> \n",
      "-2.9431826714503764 i <UNK> \n",
      "-2.9745844378502313 the <UNK> \n",
      "-3.231045927650467 he <UNK> \n",
      "-3.6798788065850516 is was \n",
      "-3.716161858555806 is pynchon \n",
      "-3.762310253403003 an was \n",
      "-3.7716870446288944 a was \n",
      "-3.774495372388449 a a \n",
      "-4.117365722664278 <UNK> <UNK> \n",
      "-4.194280199401602 is <UNK> \n",
      "-4.22983945025578 a is \n",
      "-4.284904767007642 not <UNK> \n",
      "-4.342336593835441 the a \n",
      "-4.4072100488029875 thomas is \n",
      "-4.505893695789199 hal is \n",
      "-4.636813397007328 an <UNK> \n",
      "-4.720762499795603 a one \n",
      "-4.765228854988761 <UNK> was \n",
      "-4.8147002630809315 an is \n",
      "-5.18374898026554 my <UNK> \n",
      "-5.192523684024364 he a \n",
      "-5.290430943631914 you <UNK> \n",
      "-5.298313092893368 the is \n",
      "-5.3100206966376104 what <UNK> \n",
      "-5.466648565043418 a for \n",
      "-5.665939842441504 is emotion \n",
      "-5.800381876131437 do <UNK> \n",
      "-5.944787645571135 a s \n",
      "\n",
      "===========\n",
      "-1.404419861326226 <UNK> is a \n",
      "-2.0486640006210606 a <UNK> <UNK> \n",
      "-2.588811728843661 a <UNK> <EOS> \n",
      "-3.6235927986461185 he <UNK> system \n",
      "-3.686200027306518 is was a \n",
      "-3.7564045011309855 i <UNK> system \n",
      "-3.763085294842832 an was a \n",
      "-3.773772698451658 a was a \n",
      "-3.7886167228740795 a a a \n",
      "-3.8687597142003836 the <UNK> <EOS> \n",
      "-4.14976689737794 is pynchon <UNK> \n",
      "-4.240955730399363 <UNK> <UNK> <EOS> \n",
      "-4.2452261424505275 a is a \n",
      "-4.301192471756874 the <UNK> <UNK> \n",
      "-4.343557266818275 the a a \n",
      "-4.411169246810173 thomas is a \n",
      "-4.540967255702771 hal is a \n",
      "-4.610911985564253 i <UNK> <EOS> \n",
      "-4.648989843202142 i <UNK> <UNK> \n",
      "-4.721610556552097 a one a \n",
      "-4.785353634094351 <UNK> was a \n",
      "-4.819577964926688 an is a \n",
      "-4.8814020156303295 is <UNK> <EOS> \n",
      "-4.900886949923962 a <UNK> system \n",
      "-4.94255198335884 a <UNK> the \n",
      "-4.988494755125342 is <UNK> <UNK> \n",
      "-5.068327719267522 is pynchon <EOS> \n",
      "-5.1276989024158555 an <UNK> <EOS> \n",
      "-5.177303412298327 a <UNK> is \n",
      "-5.191790141243111 the <UNK> system \n",
      "\n",
      "===========\n",
      "-1.422110027322249 <UNK> is a <UNK> \n",
      "-2.6719260845797024 a <UNK> <UNK> <EOS> \n",
      "-2.9020539045114684 a <UNK> <EOS> <PAD> \n",
      "-2.9297815398070255 a <UNK> <UNK> wide \n",
      "-3.9533774157421555 is was a <UNK> \n",
      "-4.041048724304452 an was a <UNK> \n",
      "-4.257703431721865 <UNK> <UNK> <EOS> <PAD> \n",
      "-4.291522480520695 i <UNK> system in \n",
      "-4.298900518498757 the <UNK> <EOS> <PAD> \n",
      "-4.382023342998028 he <UNK> system in \n",
      "-4.423254584810904 the <UNK> <UNK> <EOS> \n",
      "-4.542307446293703 a <UNK> <EOS> few \n",
      "-4.557582575432006 a was a a \n",
      "-4.617245970279324 he <UNK> system few \n",
      "-4.65691558324836 is pynchon <UNK> wide \n",
      "-4.665785124853381 hal is a <UNK> \n",
      "-4.7861678167249675 a a a a \n",
      "-4.91495634631406 a was a <UNK> \n",
      "-4.921277697867712 is <UNK> <EOS> <PAD> \n",
      "-4.9840934442904645 i <UNK> <EOS> <PAD> \n",
      "-4.993791211462886 the a a <UNK> \n",
      "-4.996173681879651 an is a <UNK> \n",
      "-5.057209538543452 a is a <UNK> \n",
      "-5.067577018590257 <UNK> was a <UNK> \n",
      "-5.080131717985764 is pynchon <EOS> <PAD> \n",
      "-5.097640592322647 is <UNK> <UNK> <EOS> \n",
      "-5.123840199614744 a <UNK> <EOS> in \n",
      "-5.222543469593978 a a a <UNK> \n",
      "-5.2230648204227315 a <UNK> the a \n",
      "-5.231595005202203 i <UNK> <UNK> <EOS> \n",
      "\n",
      "===========\n",
      "-2.264763546751481 <UNK> is a <UNK> <EOS> \n",
      "-2.3452968085167463 <UNK> is a <UNK> that \n",
      "-2.7702594173370825 a <UNK> <UNK> <EOS> <PAD> \n",
      "-2.9021972043444393 a <UNK> <EOS> <PAD> <PAD> \n",
      "-3.4196593797978156 a <UNK> <UNK> wide of \n",
      "-3.960501646106583 is was a <UNK> <EOS> \n",
      "-4.055416890106043 <UNK> is a <UNK> of \n",
      "-4.120702393640962 an was a <UNK> <EOS> \n",
      "-4.257712253248201 <UNK> <UNK> <EOS> <PAD> <PAD> \n",
      "-4.293567340207654 i <UNK> system in <UNK> \n",
      "-4.299034399492584 the <UNK> <EOS> <PAD> <PAD> \n",
      "-4.385890389958574 he <UNK> system in <UNK> \n",
      "-4.547703623275831 a <UNK> <EOS> few <UNK> \n",
      "-4.60587203899298 the <UNK> <UNK> <EOS> <PAD> \n",
      "-4.620928920091817 he <UNK> system few <UNK> \n",
      "-4.629474062429012 a was a a <UNK> \n",
      "-4.668620540287923 hal is a <UNK> <EOS> \n",
      "-4.833652683801914 a a a a <UNK> \n",
      "-4.921292718351002 is <UNK> <EOS> <PAD> <PAD> \n",
      "-4.965802461315951 a <UNK> <UNK> wide which \n",
      "-4.978108043430765 a <UNK> <UNK> wide <UNK> \n",
      "-4.984868127825205 i <UNK> <EOS> <PAD> <PAD> \n",
      "-5.011736407478098 an is a <UNK> <EOS> \n",
      "-5.080155560127896 is pynchon <EOS> <PAD> <PAD> \n",
      "-5.092059629505295 is pynchon <UNK> wide <EOS> \n",
      "-5.096533125068156 a is a <UNK> <EOS> \n",
      "-5.1008294534407455 is <UNK> <UNK> <EOS> <PAD> \n",
      "-5.131365996574612 a <UNK> <EOS> in <UNK> \n",
      "-5.163042801057498 <UNK> is a <UNK> by \n",
      "-5.171989621545585 <UNK> is a <UNK> <UNK> \n",
      "\n",
      "===========\n",
      "-2.7702612054780245 a <UNK> <UNK> <EOS> <PAD> <PAD> \n",
      "-2.9021983964380453 a <UNK> <EOS> <PAD> <PAD> <PAD> \n",
      "-3.4140534719900977 <UNK> is a <UNK> that the \n",
      "-3.4912675656435 <UNK> is a <UNK> <EOS> <PAD> \n",
      "-3.500739989802211 <UNK> is a <UNK> <EOS> of \n",
      "-3.6752346404832323 <UNK> is a <UNK> that <UNK> \n",
      "-3.720000836170658 <UNK> is a <UNK> <EOS> <EOS> \n",
      "-3.9049568058663464 a <UNK> <UNK> wide of <EOS> \n",
      "-3.9687569558611173 is was a <UNK> <EOS> <PAD> \n",
      "-4.077045814400131 <UNK> is a <UNK> that <EOS> \n",
      "-4.187480021510603 <UNK> is a <UNK> of <EOS> \n",
      "-4.257712730085473 <UNK> <UNK> <EOS> <PAD> <PAD> <PAD> \n",
      "-4.296461933542818 i <UNK> system in <UNK> <EOS> \n",
      "-4.29903559158619 the <UNK> <EOS> <PAD> <PAD> <PAD> \n",
      "-4.304137603611161 an was a <UNK> <EOS> <PAD> \n",
      "-4.404927452677577 he <UNK> system in <UNK> <EOS> \n",
      "-4.415397382339232 a <UNK> <UNK> wide of <UNK> \n",
      "-4.605873469505478 the <UNK> <UNK> <EOS> <PAD> <PAD> \n",
      "-4.669083656390248 hal is a <UNK> <EOS> <PAD> \n",
      "-4.86731440973045 he <UNK> system few <UNK> <EOS> \n",
      "-4.896138563617804 a was a a <UNK> <EOS> \n",
      "-4.921293195188274 is <UNK> <EOS> <PAD> <PAD> <PAD> \n",
      "-4.979549905492607 a <UNK> <UNK> wide <UNK> <EOS> \n",
      "-4.98486943912825 i <UNK> <EOS> <PAD> <PAD> <PAD> \n",
      "-5.0025812099559674 a a a a <UNK> <EOS> \n",
      "-5.017398516587607 an is a <UNK> <EOS> <PAD> \n",
      "-5.02637902504892 a <UNK> <EOS> few <UNK> <EOS> \n",
      "-5.08015687143094 is pynchon <EOS> <PAD> <PAD> <PAD> \n",
      "-5.100829691859353 is <UNK> <UNK> <EOS> <PAD> <PAD> \n",
      "-5.121858183448853 a is a <UNK> <EOS> <PAD> \n",
      "\n",
      "===========\n",
      "-2.770261324687321 a <UNK> <UNK> <EOS> <PAD> <PAD> <PAD> \n",
      "-2.9021983964380453 a <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> \n",
      "-3.4912728108659965 <UNK> is a <UNK> <EOS> <PAD> <PAD> \n",
      "-3.7200754639705016 <UNK> is a <UNK> <EOS> <EOS> <PAD> \n",
      "-3.785759262196524 <UNK> is a <UNK> that <UNK> <UNK> \n",
      "-3.9687569558611173 is was a <UNK> <EOS> <PAD> <PAD> \n",
      "-4.196458237454529 a <UNK> <UNK> wide of <EOS> <PAD> \n",
      "-4.257712730085473 <UNK> <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> \n",
      "-4.29903559158619 the <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> \n",
      "-4.3041379612390935 an was a <UNK> <EOS> <PAD> <PAD> \n",
      "-4.350381227026616 i <UNK> system in <UNK> <EOS> <PAD> \n",
      "-4.42040979956212 a <UNK> <UNK> wide of <UNK> <EOS> \n",
      "-4.522375736058775 <UNK> is a <UNK> that the <UNK> \n",
      "-4.605873588714775 the <UNK> <UNK> <EOS> <PAD> <PAD> <PAD> \n",
      "-4.669083656390248 hal is a <UNK> <EOS> <PAD> <PAD> \n",
      "-4.676987481220298 <UNK> is a <UNK> of <EOS> <PAD> \n",
      "-4.921293195188274 is <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> \n",
      "-4.923611342669454 a was a a <UNK> <EOS> <PAD> \n",
      "-4.98486943912825 i <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> \n",
      "-4.987623673444749 a <UNK> <UNK> wide <UNK> <EOS> <PAD> \n",
      "-5.017398516587607 an is a <UNK> <EOS> <PAD> <PAD> \n",
      "-5.022079230005084 a a a a <UNK> <EOS> <PAD> \n",
      "-5.080156990640237 is pynchon <EOS> <PAD> <PAD> <PAD> <PAD> \n",
      "-5.100829691859353 is <UNK> <UNK> <EOS> <PAD> <PAD> <PAD> \n",
      "-5.121853324113017 <UNK> is a <UNK> that <EOS> <PAD> \n",
      "-5.1218584218674605 a is a <UNK> <EOS> <PAD> <PAD> \n",
      "-5.345111194636847 he <UNK> system in <UNK> <EOS> <PAD> \n",
      "-5.403080045897547 he <UNK> system few <UNK> <EOS> <UNK> \n",
      "-5.446975066757226 <UNK> is a <UNK> <EOS> of emotion \n",
      "-5.554246586596806 <UNK> is a <UNK> that <EOS> <UNK> \n",
      "\n",
      "===========\n",
      "-2.770261324687321  a <UNK> <UNK> <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "-2.9021983964380453  a <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "-3.491272930075293  <UNK> is a <UNK> <EOS> <PAD> <PAD> <PAD>\n",
      "-3.7200755831797983  <UNK> is a <UNK> <EOS> <EOS> <PAD> <PAD>\n",
      "-3.7949160986549537  <UNK> is a <UNK> that <UNK> <UNK> <EOS>\n",
      "-3.968757075070414  is was a <UNK> <EOS> <PAD> <PAD> <PAD>\n",
      "-4.196458237454529  a <UNK> <UNK> wide of <EOS> <PAD> <PAD>\n",
      "-4.257712730085473  <UNK> <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "-4.29903559158619  the <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "-4.30413808044839  an was a <UNK> <EOS> <PAD> <PAD> <PAD>\n",
      "-4.350383015167558  i <UNK> system in <UNK> <EOS> <PAD> <PAD>\n",
      "-4.42195200663838  a <UNK> <UNK> wide of <UNK> <EOS> <PAD>\n",
      "-4.543405445716134  <UNK> is a <UNK> that the <UNK> <EOS>\n",
      "-4.605873588714775  the <UNK> <UNK> <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "-4.669083775599544  hal is a <UNK> <EOS> <PAD> <PAD> <PAD>\n",
      "-4.676987481220298  <UNK> is a <UNK> of <EOS> <PAD> <PAD>\n",
      "-4.921293314397571  is <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "-4.923611581088061  a was a a <UNK> <EOS> <PAD> <PAD>\n",
      "-4.98486943912825  i <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "-4.987623673444749  a <UNK> <UNK> wide <UNK> <EOS> <PAD> <PAD>\n",
      "-5.017398635796904  an is a <UNK> <EOS> <PAD> <PAD> <PAD>\n",
      "-5.022079468423692  a a a a <UNK> <EOS> <PAD> <PAD>\n",
      "-5.080157109849534  is pynchon <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "-5.100829691859353  is <UNK> <UNK> <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "-5.121854158578392  <UNK> is a <UNK> that <EOS> <PAD> <PAD>\n",
      "-5.121858541076757  a is a <UNK> <EOS> <PAD> <PAD> <PAD>\n",
      "-5.34511572460011  he <UNK> system in <UNK> <EOS> <PAD> <PAD>\n",
      "-5.448506349408331  <UNK> is a <UNK> <EOS> of emotion <EOS>\n",
      "-5.562333393692367  <UNK> is a <UNK> that <EOS> <UNK> <EOS>\n",
      "-6.702389226612315  he <UNK> system few <UNK> <EOS> <UNK> and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prep_input(text,max_len=8):\n",
    "    out_text = \"\"\n",
    "    tab = []\n",
    "    for word in text.split():\n",
    "        tab.insert(0,word)\n",
    "    tab.insert(0,\"<EOS>\")\n",
    "    while len(tab) < max_len:\n",
    "        tab.insert(0,\"<PAD>\")\n",
    "    return \" \".join([word for word in tab])\n",
    "\n",
    "input_text = prep_input(\"what is baseketball\",8)\n",
    "sentence = s2s.eval_seq(input_text,8,30)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# needed to [seq] the whole input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
