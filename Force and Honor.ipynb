{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Force and) Honor Project\n",
    "\n",
    "## Short introduction\n",
    "\n",
    "Well, you know why we are here. In this notebook, I detail how I create a chatbot using first a multilayer bi-LSTM connected to a simple [Glove embeddings](https://nlp.stanford.edu/projects/glove/), and then maybe, if I have time, using a more complexe approach.\n",
    "\n",
    "First things first, let's start with the data.\n",
    "\n",
    "## Data Gathering and Preprocessing\n",
    "\n",
    "The first model will use Cornell dataset provided by the organiser. It's a small dataset so will give me probably a lesser quality bot, but it will be trained faster and will help for prototyping.\n",
    "Rather than making it download again on your machine, I will suppose it is on your machine under `./data/cornell`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:02<00:00, 30497.50it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "\n",
    "dataset_path = os.path.join(\"./data/cornell\")\n",
    "# Be mindful that I had to change the code in datasets.py for fast_preprocessing to \n",
    "# be actually taken into account\n",
    "data = datasets.readCornellData(dataset_path, max_len=20, fast_preprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24792\n",
      "[('there', 'where'), ('have fun tonight', 'tons'), ('what good stuff', 'the real you'), ('wow', 'lets go'), ('she okay', 'i hope so'), ('they do to', 'they do not'), ('who', 'joey'), ('its more', 'expensive'), ('hey sweet cheeks', 'hi joey'), ('whereve you been', 'nowhere hi daddy')]\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As written above, we are going to use Glove embeddings. We'll start by using the smallest version which is glove.6B.50d.txt, an 164M file of 400k vocabulary, 6B tokens projected into a 50 dimensions space (bigger versions go up to 840B tokens and 300 dimensions).\n",
    "\n",
    "Please note the treatment of special words: `<PAD>`, `<UNK>` and `<S>`. `<PAD>` is used as a padding word to put all the sentence at equal size (because tensor). `<UNK>` is a token replacing words which are not in the vocabulary. I deal here with out of vocabulary word by generating random vectors, which kind of treat them like noise. `<S>` is the starting token of the deoder. It doesn't need an embedding as the decoder output is a softmax on the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Padding value... I am not sure if I'll use it...\n",
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\"\n",
    "START = \"<S>\"\n",
    "\n",
    "class gloveEmbeddings:\n",
    "    def __init__(self):\n",
    "        self._embeddings = {}\n",
    "        self._id2word = []\n",
    "        self._word2id = {}\n",
    "        self._embeddings_dim = 0\n",
    "        \n",
    "    def load(self,filename,voc_size=0):\n",
    "        \"\"\"\n",
    "        Load the first voc_size words of a given glove size\n",
    "        if voc_size == 0, loads the whole file.\n",
    "        vocab is a set including the desired vocabulary. If not specified, we take everything\n",
    "        Returns:\n",
    "            embeddings: a dictionary word:embedding\n",
    "            word_order: a list containing the words in the same order as in the document\n",
    "            embeddings_dim: dimensionality of the embedding\n",
    "        \"\"\"\n",
    "        \n",
    "        # In case of multiple call\n",
    "        self._embeddings = {}\n",
    "        self._id2word = []\n",
    "        self._word2id = {}\n",
    "        self._embeddings_dim = 0\n",
    "        \n",
    "        self._id2word.append(PAD)\n",
    "        self._word2id[PAD] = 0\n",
    "        self._id2word.append(UNK)\n",
    "        self._word2id[UNK] = 1\n",
    "        count = 2\n",
    "        with open(filename,\"rt\") as f:\n",
    "            for line in f:\n",
    "                word,*proj = line.split()\n",
    "                self._embeddings[word] = np.array(proj,dtype=np.float32)\n",
    "                self._id2word.append(word)\n",
    "                self._word2id[word] = count\n",
    "                count += 1\n",
    "                if voc_size > 0 and count >= voc_size: break\n",
    "\n",
    "        self._embeddings_dim = len(next(iter(self._embeddings.values())))\n",
    "        self._embeddings[PAD] = np.zeros(self._embeddings_dim) # needed the dim\n",
    "        # note that UNK doesn't have an embedding, as it's a random vector generated at execution\n",
    "\n",
    "    def get(self,word):\n",
    "        \"\"\"\n",
    "        We'll deal with unknown words by returning a random vector\n",
    "        \"\"\"\n",
    "        if word not in self._embeddings:\n",
    "            return np.random.rand(self._embeddings_dim) * 2 - 1\n",
    "        return self._embeddings[word]\n",
    "    \n",
    "    def word2id(self,word):\n",
    "        if word not in self._word2id:\n",
    "            return -1\n",
    "        return self._word2id[word]\n",
    "    \n",
    "    def id2word(self,index):\n",
    "        return self._id2word[index] # might trigger out of range\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = gloveEmbeddings()\n",
    "glove.load(\"data/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove will be used in the input. As the output is only compared to the target sentences, we need to limit our output vocabulary to the set of words in the target vocabulary. We will also filter out words with less than a certain amount of occurences (most likely typos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "class outputVoc:\n",
    "    def __init__(self):\n",
    "        self._id2word = []\n",
    "        self._word2id = {}\n",
    "        self._word_freq = {}\n",
    "    \n",
    "    def learn_from_target(self,target_text,typo_limit=3):\n",
    "        self._word_freq = {}\n",
    "        for sen in target_text:\n",
    "            for word in sen.split():\n",
    "                if word not in self._word_freq:\n",
    "                    self._word_freq[word] = 1\n",
    "                else:\n",
    "                    self._word_freq[word] += 1\n",
    "        # Sanity check\n",
    "        max_word = \"\"\n",
    "        max_freq = 0\n",
    "        min_word = \"\"\n",
    "        min_freq = sys.maxsize\n",
    "        for word,freq in self._word_freq.items():\n",
    "            if freq > max_freq: \n",
    "                max_freq = freq\n",
    "                max_word = word\n",
    "            elif freq < min_freq:\n",
    "                min_freq = freq\n",
    "                min_word = word\n",
    "\n",
    "        print(\"Max freq word = \\\"{}\\\" : {}\".format(max_word,max_freq))\n",
    "        print(\"Min freq word = \\\"{}\\\" {}\".format(min_word,min_freq))\n",
    "        \n",
    "        self._id2word = []\n",
    "        self._word2id = {}\n",
    "        self._id2word.append(PAD)\n",
    "        self._word2id[PAD] = 0\n",
    "        self._id2word.append(START)\n",
    "        self._word2id[UNK] = 1\n",
    "        for word,freq in self._word_freq.items():\n",
    "            if freq >= typo_limit:\n",
    "                self._id2word.append(word)\n",
    "                self._word2id[word] = len(self._id2word) - 1\n",
    "        \n",
    "    def word2id(self,word):\n",
    "        if word not in self._word2id:\n",
    "            return -1\n",
    "        return self._word2id[word]\n",
    "    \n",
    "    def id2word(self,index):\n",
    "        return self._id2word[index] # can cause out of range exception\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max freq word = \"you\" : 2850\n",
      "Min freq word = \"megan\" 1\n",
      "Output vocabulary size 1724\n"
     ]
    }
   ],
   "source": [
    "out_voc = outputVoc()\n",
    "input_text,target_text = zip(*data)\n",
    "out_voc.learn_from_target(target_text)\n",
    "print(\"Output vocabulary size {}\".format(out_voc.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that in tensor form and split training / testing (90 / 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences        \n",
    "\n",
    "# replace words by their id in the tensor, higher in the network they will be replaced by their embeddings\n",
    "input_tensor,target_tensor = zip(*data)\n",
    "input_tensor = [[glove.word2id(word) for word in sentence.split()] for sentence in input_tensor]\n",
    "target_tensor = [[out_voc.word2id(word) for word in sentence.split()] for sentence in target_tensor]\n",
    "\n",
    "input_tensor = pad_sequences(input_tensor, maxlen=None,\n",
    "                             dtype='int32', padding='post', value=glove.word2id(PAD))\n",
    "target_tensor = pad_sequences(target_tensor, maxlen=None,\n",
    "                             dtype='int32', padding='post', value=out_voc.word2id(PAD))\n",
    "\n",
    "# train - eval split at 90-10\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 104 7193    0    0    0    0    0] <-> ['what', 'dame', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "[  43  348    9 1858    0    0    0] <-> ['i', 'know', 'a', 'guy', '<PAD>', '<PAD>', '<PAD>']\n",
      "[810   0   0   0   0   0   0] <-> ['mother', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "[ 37   8 837   0   0   0   0] <-> ['were', 'in', 'love', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "[86  0  0  0  0  0  0] <-> ['no', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "==================================================\n",
      "['what', 'dame', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] :: ['paine', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['i', 'know', 'a', 'guy', '<PAD>', '<PAD>', '<PAD>'] :: ['lead', 'the', 'way', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['mother', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] :: ['hang', 'shit', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['were', 'in', 'love', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] :: ['sure', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['no', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] :: ['then', 'you', 'kill', 'her', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "for i in range(5):\n",
    "    sentence = [glove.id2word(x) for x in input_tensor_train[i]]\n",
    "    print(\"{} <-> {}\".format(input_tensor_train[i],sentence))\n",
    "# Sanity check QA still align\n",
    "print(\"=\"*50)\n",
    "for i in range(5):\n",
    "    q = [glove.id2word(x) for x in input_tensor_train[i]]\n",
    "    a = [out_voc.id2word(x) for x in target_tensor_train[i]]\n",
    "    print(\"{} :: {}\".format(q,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM building\n",
    "\n",
    "Here come the big thing. Coding a bi-LSTM connected to the embeddings.\n",
    "First we define the tensorflow dataset as input with mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "lstm_dim = 256\n",
    "#units = 1024\n",
    "#vocab_inp_size = len(inp_lang.word2idx)\n",
    "#vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "def LSTM(dim):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "    if tf.test.is_gpu_available():\n",
    "        print(\"GPU acceleration available. LSTM cells will run faster\")\n",
    "        return tf.keras.layers.CuDNNLSTM(dim, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True)\n",
    "    else:\n",
    "        print(\"GPU acceleration not available. LSTM cells will be slow\")\n",
    "        return tf.keras.layers.LSTM(dim,\n",
    "                               return_sequences=True, \n",
    "                               return_state=True)\n",
    "Model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "class Seq2SeqChat(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "    # You might recognize the same placeholder as in our seq2seq model in week 4\n",
    "    def declare_placeholders(self):\n",
    "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "    \n",
    "    # Placeholders for input and its actual lengths.\n",
    "    self.input_batch = tf.placeholder(shape=(None, None), dtype=tf.int32, name='input_batch')\n",
    "    self.input_batch_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='input_batch_lengths')\n",
    "    \n",
    "    # Placeholders for groundtruth and its actual lengths.\n",
    "    self.ground_truth = tf.placeholder(shape=(None,None), dtype=tf.int32, name=\"ground_truth\")\n",
    "    self.ground_truth_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='ground_truth_lengths')\n",
    "        \n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    self.learning_rate_ph = tf.placeholder_with_default(tf.cast(0.001,tf.float32),shape=[])\n",
    "\n",
    "def create_embeddings(self, vocab_size, embeddings_size):\n",
    "    \"\"\"Specifies embeddings layer and embeds an input batch.\"\"\"\n",
    "     \n",
    "    random_initializer = tf.random_uniform((vocab_size, embeddings_size), -1.0, 1.0)\n",
    "    self.embeddings = tf.Variable(random_initializer,dtype=tf.float32,name=\"embedding_matrix\")\n",
    "    \n",
    "    # Perform embeddings lookup for self.input_batch. \n",
    "    self.input_batch_embedded = tf.nn.embedding_lookup(self.embeddings,\n",
    "                                                       self.input_batch,\n",
    "                                                      name=\"batch_embedded\")\n",
    "def build_encoder(self, hidden_size):\n",
    "    # type of cell\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(\n",
    "                        LSTM(self.input_batch_lengths, activation='relu', return_sequences=True,dropout=dropout),\n",
    "                        merge_mode='concat',\n",
    "                        input_shape=(None, input_size),\n",
    "                        batch_input_shape=(batch_size, None, input_size)))\n",
    "    model.add(Bidirectional(LSTM(output_size, activation='relu', return_sequences=True,\n",
    "                             dropout=dropout), merge_mode='sum'))\n",
    "    model.add(Bidirectional(LSTM(output_size, activation='relu', return_sequences=True,\n",
    "                             dropout=dropout), merge_mode='sum'))\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.001, clipnorm=1), metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
